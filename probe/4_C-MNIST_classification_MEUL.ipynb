{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#%config InlineBackend.figure_format='svg'\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "import shl_scripts.shl_tools\n",
    "from shl_scripts.shl_experiments import SHL\n",
    "\n",
    "import DNN.mnist_loader as data_loader\n",
    "import DNN.network as network\n",
    "from shl_scripts.shl_encode import sparse_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data,validation_data,test_data=data_loader.load_data()\n",
    "training_image = training_data[0]\n",
    "training_supervision = training_data[1]\n",
    "test_image = test_data[0]\n",
    "test_supervision = test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag ='2017-06-01_MNIST_MEUL_DEBUG_'\n",
    "DEBUG_DOWNSCALE, verbose = 10, 10\n",
    "tag ='2017-06-01_MNIST_MEUL_'\n",
    "DEBUG_DOWNSCALE, verbose = 1, 10\n",
    "patch_size = (28,28)\n",
    "n_dictionary = 15**2\n",
    "l0_sparseness = 7\n",
    "n_iter = 2**14\n",
    "eta = 0.01\n",
    "eta_homeo = 0.01\n",
    "verbose = 0\n",
    "list_figures=['show_dico']\n",
    "n_hidden = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the computation is locked /tmp/data_cache/2017-06-01_MNIST_MEUL_n_dictionary144_coding.npy_lock_pid-29675_host-emmy\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'sparse_code' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2180c4df1f87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdico\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_dico\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmatname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist_figures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist_figures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m## formating the date to fit theano standard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtraining_sparse_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdico\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdico\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmatname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     test_sparse_vector = sparse_encode(test_image, dico.dictionary, algorithm = shl.learning_algorithm,\n",
      "\u001b[0;32m/Users/laurentperrinet/pool/science/BICV/SHL_scripts/shl_scripts/shl_experiments.py\u001b[0m in \u001b[0;36mcode\u001b[0;34m(self, data, dico, coding_algorithm, matname, l0_sparseness, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0msparse_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmatname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparse_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdico\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'sparse_code' referenced before assignment"
     ]
    }
   ],
   "source": [
    "for n_dictionary_ in np.arange(12, 30, 3)**2:\n",
    "    \n",
    "    shl = SHL(DEBUG_DOWNSCALE=DEBUG_DOWNSCALE, \n",
    "                eta=eta, eta_homeo=eta_homeo, verbose=verbose,\n",
    "                n_iter=n_iter, patch_size=patch_size, l0_sparseness=l0_sparseness,\n",
    "                n_dictionary=n_dictionary_)\n",
    "    matname = tag + 'n_dictionary' + str(n_dictionary_)\n",
    "    dico = shl.learn_dico(data=training_image, matname=matname,list_figures=list_figures)    \n",
    "    ## formating the date to fit theano standard\n",
    "    training_sparse_vector = shl.code(data=training_image, dico=dico, matname=matname)\n",
    "\n",
    "    test_sparse_vector = sparse_encode(test_image, dico.dictionary, algorithm = shl.learning_algorithm,\n",
    "                                l0_sparseness=l0_sparseness, fit_tol = None,\n",
    "                                P_cum = dico.P_cum, verbose = 0)\n",
    "    wrapped_training_data = (training_sparse_vector, training_supervision)\n",
    "    wrapped_test_data = (test_sparse_vector, test_supervision)\n",
    "    \n",
    "    wrapped_inputs = [np.reshape(x, (n_dictionary_, 1)) for x in wrapped_training_data[0]]\n",
    "    wrapped_results = [vectorized_result(y) for y in wrapped_training_data[1]]\n",
    "    wrapped_training_data = zip(wrapped_inputs, wrapped_results)\n",
    "    wrapped_test_inputs = [np.reshape(x, (n_dictionary_, 1)) for x in wrapped_test_data[0]]\n",
    "    wrapped_test_data_final = zip(wrapped_test_inputs, wrapped_test_data[1])\n",
    "    \n",
    "    print(\" ----- learning for the dico of size : {0} -----\".format(n_dictionary_))\n",
    "    ## running the network\n",
    "    net=network.Network([n_dictionary_, n_hidden, 10])\n",
    "    net.SGD(training_data=wrapped_training_data,\n",
    "       epochs=30,\n",
    "       mini_batch_size=10,\n",
    "       eta=3.0,\n",
    "       test_data=wrapped_test_data_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for l0_sparseness_ in np.arange(5, 40, 5):\n",
    "    \n",
    "    shl = SHL(DEBUG_DOWNSCALE=DEBUG_DOWNSCALE, \n",
    "                eta=eta, eta_homeo=eta_homeo, verbose=verbose,\n",
    "                n_iter=n_iter, patch_size=patch_size, l0_sparseness=l0_sparseness_,\n",
    "                n_dictionary=n_dictionary)\n",
    "    matname = tag + 'l0_sparseness=' + str(l0_sparseness_)\n",
    "    dico = shl.learn_dico(data=training_image,matname=matname, list_figures=list_figures)\n",
    "    \n",
    "    training_sparse_vector = shl.code(data=training_image, dico=dico, matname=matname)\n",
    "    \n",
    "    test_sparse_vector = sparse_encode(test_image, dico.dictionary, algorithm=shl.learning_algorithm,\n",
    "                                l0_sparseness=l0_sparseness_, fit_tol=None,\n",
    "                                P_cum=dico.P_cum, verbose = 0)\n",
    "    wrapped_training_data = (training_sparse_vector, training_supervision)\n",
    "    wrapped_test_data = (test_sparse_vector, test_supervision)\n",
    "    \n",
    "    wrapped_inputs = [np.reshape(x, (shl.n_dictionary, 1)) for x in wrapped_training_data[0]]\n",
    "    wrapped_results = [vectorized_result(y) for y in wrapped_training_data[1]]\n",
    "    wrapped_training_data = zip(wrapped_inputs, wrapped_results)\n",
    "    wrapped_test_inputs = [np.reshape(x, (shl.n_dictionary, 1)) for x in wrapped_test_data[0]]\n",
    "    wrapped_test_data_final = zip(wrapped_test_inputs, wrapped_test_data[1])\n",
    "    \n",
    "    print(\" ----- learning for the dico of sparseness: {0} -----\".format(l0_sparseness_))\n",
    "    ## running the network\n",
    "    net=network.Network([shl.n_dictionary, n_hidden, 10])\n",
    "    net.SGD(training_data=wrapped_training_data,\n",
    "       epochs=30,\n",
    "       mini_batch_size=10,\n",
    "       eta=3.0,\n",
    "       test_data=wrapped_test_data_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
