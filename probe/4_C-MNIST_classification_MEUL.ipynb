{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#%config InlineBackend.figure_format='svg'\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "import shl_scripts.shl_tools\n",
    "from shl_scripts.shl_experiments import SHL\n",
    "\n",
    "import DNN.mnist_loader as data_loader\n",
    "import DNN.network as network\n",
    "from shl_scripts.shl_encode import sparse_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data,validation_data,test_data=data_loader.load_data()\n",
    "training_image = training_data[0]\n",
    "training_supervision = training_data[1]\n",
    "test_image = test_data[0]\n",
    "test_supervision = test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag ='2017-05-26_MNIST_MEUL '\n",
    "DEBUG_DOWNSCALE, verbose = 1, 10\n",
    "tag ='2017-05-30_MNIST_MEUL_DEBUG'\n",
    "DEBUG_DOWNSCALE, verbose = 10, 10\n",
    "patch_size = (28,28)\n",
    "n_dictionary = 15**2\n",
    "l0_sparseness = 7\n",
    "n_iter = 2**16\n",
    "eta = 0.01\n",
    "eta_homeo = 0.01\n",
    "verbose = 0\n",
    "list_figures=['show_dico']\n",
    "n_hidden = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ----- learning for the dico of size : 144 -----\n",
      "Epoch 0: 5633 / 10000\n",
      "Epoch 1: 5916 / 10000\n",
      "Epoch 2: 5858 / 10000\n",
      "Epoch 3: 5830 / 10000\n",
      "Epoch 4: 5851 / 10000\n",
      "Epoch 5: 5848 / 10000\n",
      "Epoch 6: 5940 / 10000\n",
      "Epoch 7: 5835 / 10000\n",
      "Epoch 8: 5845 / 10000\n",
      "Epoch 9: 5808 / 10000\n",
      "Epoch 10: 5748 / 10000\n",
      "Epoch 11: 5727 / 10000\n",
      "Epoch 12: 5783 / 10000\n",
      "Epoch 13: 5832 / 10000\n",
      "Epoch 14: 5730 / 10000\n",
      "Epoch 15: 5820 / 10000\n",
      "Epoch 16: 5792 / 10000\n",
      "Epoch 17: 5807 / 10000\n",
      "Epoch 18: 5858 / 10000\n",
      "Epoch 19: 5775 / 10000\n",
      "Epoch 20: 5757 / 10000\n",
      "Epoch 21: 5729 / 10000\n",
      "Epoch 22: 5757 / 10000\n",
      "Epoch 23: 5745 / 10000\n",
      "Epoch 24: 5781 / 10000\n",
      "Epoch 25: 5808 / 10000\n",
      "Epoch 26: 5723 / 10000\n",
      "Epoch 27: 5778 / 10000\n",
      "Epoch 28: 5873 / 10000\n",
      "Epoch 29: 5815 / 10000\n",
      " ----- learning for the dico of size : 225 -----\n",
      "Epoch 0: 3567 / 10000\n",
      "Epoch 1: 4464 / 10000\n",
      "Epoch 2: 4258 / 10000\n",
      "Epoch 3: 4268 / 10000\n",
      "Epoch 4: 4107 / 10000\n",
      "Epoch 5: 4307 / 10000\n",
      "Epoch 6: 4199 / 10000\n",
      "Epoch 7: 4156 / 10000\n",
      "Epoch 8: 4140 / 10000\n",
      "Epoch 9: 4133 / 10000\n",
      "Epoch 10: 4125 / 10000\n",
      "Epoch 11: 4190 / 10000\n",
      "Epoch 12: 4152 / 10000\n",
      "Epoch 13: 4123 / 10000\n",
      "Epoch 14: 4144 / 10000\n",
      "Epoch 15: 4108 / 10000\n",
      "Epoch 16: 4113 / 10000\n",
      "Epoch 17: 4052 / 10000\n",
      "Epoch 18: 4025 / 10000\n",
      "Epoch 19: 4042 / 10000\n",
      "Epoch 20: 4003 / 10000\n",
      "Epoch 21: 3989 / 10000\n",
      "Epoch 22: 3985 / 10000\n",
      "Epoch 23: 3963 / 10000\n",
      "Epoch 24: 3967 / 10000\n",
      "Epoch 25: 3950 / 10000\n",
      "Epoch 26: 3937 / 10000\n",
      "Epoch 27: 3953 / 10000\n",
      "Epoch 28: 3922 / 10000\n",
      "Epoch 29: 3928 / 10000\n"
     ]
    }
   ],
   "source": [
    "for n_dictionary_ in np.arange(12, 30, 3)**2:\n",
    "    \n",
    "    shl = SHL(DEBUG_DOWNSCALE=DEBUG_DOWNSCALE, \n",
    "                eta=eta, eta_homeo=eta_homeo, verbose=verbose,\n",
    "                n_iter=n_iter, patch_size=patch_size, l0_sparseness=l0_sparseness,\n",
    "                n_dictionary=n_dictionary_)\n",
    "    matname = tag + 'n_dictionary' + str(n_dictionary_)\n",
    "    dico = shl.learn_dico(data=training_image, matname=matname,list_figures=list_figures)    \n",
    "    ## formating the date to fit theano standard\n",
    "    training_sparse_vector = shl.code(data=training_image, dico=dico, matname=matname)\n",
    "\n",
    "    test_sparse_vector = sparse_encode(test_image, dico.dictionary, algorithm = shl.learning_algorithm,\n",
    "                                l0_sparseness=l0_sparseness, fit_tol = None,\n",
    "                                P_cum = dico.P_cum, verbose = 0)\n",
    "    wrapped_training_data = (training_sparse_vector, training_supervision)\n",
    "    wrapped_test_data = (test_sparse_vector, test_supervision)\n",
    "    \n",
    "    wrapped_inputs = [np.reshape(x, (n_dictionary_, 1)) for x in wrapped_training_data[0]]\n",
    "    wrapped_results = [vectorized_result(y) for y in wrapped_training_data[1]]\n",
    "    wrapped_training_data = zip(wrapped_inputs, wrapped_results)\n",
    "    wrapped_test_inputs = [np.reshape(x, (n_dictionary_, 1)) for x in wrapped_test_data[0]]\n",
    "    wrapped_test_data_final = zip(wrapped_test_inputs, wrapped_test_data[1])\n",
    "    \n",
    "    print(\" ----- learning for the dico of size : {0} -----\".format(n_dictionary_))\n",
    "    ## running the network\n",
    "    net=network.Network([n_dictionary_, n_hidden, 10])\n",
    "    net.SGD(training_data=wrapped_training_data,\n",
    "       epochs=30,\n",
    "       mini_batch_size=10,\n",
    "       eta=3.0,\n",
    "       test_data=wrapped_test_data_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for l0_sparseness_ in np.arange(5, 40, 5):\n",
    "    \n",
    "    shl = SHL(DEBUG_DOWNSCALE=DEBUG_DOWNSCALE, \n",
    "                eta=eta, eta_homeo=eta_homeo, verbose=verbose,\n",
    "                n_iter=n_iter, patch_size=patch_size, l0_sparseness=l0_sparseness_,\n",
    "                n_dictionary=n_dictionary)\n",
    "    matname = tag + 'l0_sparseness=' + str(l0_sparseness_)\n",
    "    dico = shl.learn_dico(data=training_image,matname=matname, list_figures=list_figures)\n",
    "    \n",
    "    training_sparse_vector = shl.code(data=training_image, dico=dico, matname=matname)\n",
    "    \n",
    "    test_sparse_vector = sparse_encode(test_image, dico.dictionary, algorithm=shl.learning_algorithm,\n",
    "                                l0_sparseness=l0_sparseness_, fit_tol=None,\n",
    "                                P_cum=dico.P_cum, verbose = 0)\n",
    "    wrapped_training_data = (training_sparse_vector, training_supervision)\n",
    "    wrapped_test_data = (test_sparse_vector, test_supervision)\n",
    "    \n",
    "    wrapped_inputs = [np.reshape(x, (shl.n_dictionary, 1)) for x in wrapped_training_data[0]]\n",
    "    wrapped_results = [vectorized_result(y) for y in wrapped_training_data[1]]\n",
    "    wrapped_training_data = zip(wrapped_inputs, wrapped_results)\n",
    "    wrapped_test_inputs = [np.reshape(x, (shl.n_dictionary, 1)) for x in wrapped_test_data[0]]\n",
    "    wrapped_test_data_final = zip(wrapped_test_inputs, wrapped_test_data[1])\n",
    "    \n",
    "    print(\" ----- learning for the dico of sparseness: {0} -----\".format(l0_sparseness))\n",
    "    ## running the network\n",
    "    net=network.Network([shl.n_dictionary, n_hidden, 10])\n",
    "    net.SGD(training_data=wrapped_training_data,\n",
    "       epochs=30,\n",
    "       mini_batch_size=10,\n",
    "       eta=3.0,\n",
    "       test_data=wrapped_test_data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_sparse_vector = shl.coding\n",
    "test_sparse_vector = sparse_encode(test_image, dico.dictionary, algorithm = shl.learning_algorithm,\n",
    "                            l0_sparseness = l0_sparseness, fit_tol = None,\n",
    "                            P_cum = dico.P_cum, verbose = 0)\n",
    "wrapped_training_data = (training_sparse_vector, training_supervision)\n",
    "wrapped_test_data = (test_sparse_vector, test_supervision)\n",
    "\n",
    "wrapped_inputs = [np.reshape(x, (shl.n_dictionary, 1)) for x in wrapped_training_data[0]]\n",
    "wrapped_results = [vectorized_result(y) for y in wrapped_training_data[1]]\n",
    "wrapped_training_data = zip(wrapped_inputs, wrapped_results)\n",
    "wrapped_test_inputs = [np.reshape(x, (shl.n_dictionary, 1)) for x in wrapped_test_data[0]]\n",
    "wrapped_test_data_final = zip(wrapped_test_inputs, wrapped_test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net.evaluate(list(wrapped_test_data_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(wrapped_test_data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "2**range(4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shl = SHL(DEBUG_DOWNSCALE=DEBUG_DOWNSCALE, \n",
    "            eta=eta, eta_homeo=eta_homeo, verbose=verbose,\n",
    "            n_iter=n_iter, patch_size=patch_size, l0_sparseness=l0_sparseness,\n",
    "            n_dictionary=n_dictionary)\n",
    "matname = tag\n",
    "dico = shl.learn_dico(data=training_image, matname=matname, list_figures=list_figures)    \n",
    "training_sparse_vector = shl.code(data=training_image, dico=dico, matname=matname)\n",
    "\n",
    "test_sparse_vector = sparse_encode(test_image, dico.dictionary, algorithm = shl.learning_algorithm,\n",
    "                            l0_sparseness=l0_sparseness, fit_tol = None,\n",
    "                            P_cum = dico.P_cum, verbose = 0)\n",
    "wrapped_training_data = (training_sparse_vector, training_supervision)\n",
    "wrapped_test_data = (test_sparse_vector, test_supervision)\n",
    "\n",
    "wrapped_inputs = [np.reshape(x, (n_dictionary_, 1)) for x in wrapped_training_data[0]]\n",
    "wrapped_results = [vectorized_result(y) for y in wrapped_training_data[1]]\n",
    "wrapped_training_data = zip(wrapped_inputs, wrapped_results)\n",
    "wrapped_test_inputs = [np.reshape(x, (n_dictionary_, 1)) for x in wrapped_test_data[0]]\n",
    "wrapped_test_data_final = zip(wrapped_test_inputs, wrapped_test_data[1])\n",
    "\n",
    "\n",
    "for epochs_ in 2**range(4, 8):\n",
    "    print(\" ----- learning with epochs : {} -----\".format(epochs_))\n",
    "    ## running the network\n",
    "    net=network.Network([shl.n_dictionary, n_hidden, 10])\n",
    "    net.SGD(training_data=wrapped_training_data,\n",
    "       epochs=epochs_,\n",
    "       mini_batch_size=10,\n",
    "       eta=3.0,\n",
    "       test_data=wrapped_test_data_final)\n",
    "    \n",
    "\n",
    "for mini_batch_size_ in 2**range(4, 8):\n",
    "    print(\" ----- learning with mini_batch_size : {} -----\".format(mini_batch_size_))\n",
    "    ## running the network\n",
    "    net=network.Network([shl.n_dictionary, n_hidden, 10])\n",
    "    net.SGD(training_data=wrapped_training_data,\n",
    "       epochs=30,\n",
    "       mini_batch_size=mini_batch_size_,\n",
    "       eta=3.0,\n",
    "       test_data=wrapped_test_data_final)\n",
    "    \n",
    "\n",
    "for eta_ in np.logspace(-1, 1, 9, base=10):\n",
    "    print(\" ----- learning with eta_ : {} -----\".format(eta_))\n",
    "    ## running the network\n",
    "    net=network.Network([shl.n_dictionary, n_hidden, 10])\n",
    "    net.SGD(training_data=wrapped_training_data,\n",
    "       epochs=30,\n",
    "       mini_batch_size=10,\n",
    "       eta=eta_,\n",
    "       test_data=wrapped_test_data_final)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
