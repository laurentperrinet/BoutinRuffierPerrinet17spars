{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#%config InlineBackend.figure_format='svg'\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "import shl_scripts.shl_tools\n",
    "from shl_scripts.shl_experiments import SHL\n",
    "\n",
    "import DNN.mnist_loader as data_loader\n",
    "import DNN.network as network\n",
    "from shl_scripts.shl_encode import sparse_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data,validation_data,test_data = data_loader.load_data()\n",
    "training_image = training_data[0]\n",
    "training_supervision = training_data[1]\n",
    "test_image = test_data[0]\n",
    "test_supervision = test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag ='2017-05-30_MNIST_MEUL_DEBUG_'\n",
    "DEBUG_DOWNSCALE, verbose = 10, 10\n",
    "tag ='2017-05-26_MNIST_MEUL_'\n",
    "DEBUG_DOWNSCALE, verbose = 1, 10\n",
    "patch_size = (28,28)\n",
    "n_dictionary = 15**2\n",
    "l0_sparseness = 7\n",
    "n_iter = 2**14\n",
    "eta = 0.01\n",
    "eta_homeo = 0.01\n",
    "verbose = 0\n",
    "list_figures=['show_dico']\n",
    "n_hidden = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shl = SHL(DEBUG_DOWNSCALE=DEBUG_DOWNSCALE, \n",
    "            eta=eta, eta_homeo=eta_homeo, verbose=verbose,\n",
    "            n_iter=n_iter, patch_size=patch_size, l0_sparseness=l0_sparseness,\n",
    "            n_dictionary=n_dictionary)\n",
    "matname = tag\n",
    "dico = shl.learn_dico(data=training_image, matname=matname,list_figures=list_figures)    \n",
    "## formating the date to fit theano standard\n",
    "training_sparse_vector = shl.code(data=training_image, dico=dico, matname=matname)\n",
    "\n",
    "test_sparse_vector = sparse_encode(test_image, dico.dictionary, algorithm = shl.learning_algorithm,\n",
    "                        l0_sparseness = l0_sparseness, fit_tol = None,\n",
    "                        P_cum = dico.P_cum, verbose = 0)\n",
    "wrapped_training_data = (training_sparse_vector, training_supervision)\n",
    "wrapped_test_data = (test_sparse_vector, test_supervision)\n",
    "\n",
    "wrapped_inputs = [np.reshape(x, (shl.n_dictionary, 1)) for x in wrapped_training_data[0]]\n",
    "wrapped_results = [vectorized_result(y) for y in wrapped_training_data[1]]\n",
    "wrapped_training_data = zip(wrapped_inputs, wrapped_results)\n",
    "wrapped_test_inputs = [np.reshape(x, (shl.n_dictionary, 1)) for x in wrapped_test_data[0]]\n",
    "wrapped_test_data_final = zip(wrapped_test_inputs, wrapped_test_data[1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(net.evaluate(list(wrapped_test_data_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(wrapped_test_data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**np.arange(4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shl = SHL(DEBUG_DOWNSCALE=DEBUG_DOWNSCALE, \n",
    "            eta=eta, eta_homeo=eta_homeo, verbose=verbose,\n",
    "            n_iter=n_iter, patch_size=patch_size, l0_sparseness=l0_sparseness,\n",
    "            n_dictionary=n_dictionary)\n",
    "matname = tag\n",
    "dico = shl.learn_dico(data=training_image, matname=matname, list_figures=list_figures)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_data(matname):\n",
    "    training_sparse_vector = shl.code(data=training_image, dico=dico, matname=matname)\n",
    "    test_sparse_vector = shl.code(data=test_image, dico=dico, matname=matname)\n",
    "    #sparse_encode(test_image, dico.dictionary, algorithm = shl.learning_algorithm,\n",
    "    #                        l0_sparseness=l0_sparseness, fit_tol = None,\n",
    "    #                        P_cum = dico.P_cum, verbose = 0)\n",
    "    wrapped_training_data = (training_sparse_vector, training_supervision)\n",
    "    wrapped_test_data = (test_sparse_vector, test_supervision)\n",
    "\n",
    "    wrapped_inputs = [np.reshape(x, (n_dictionary, 1)) for x in wrapped_training_data[0]]\n",
    "    wrapped_results = [vectorized_result(y) for y in wrapped_training_data[1]]\n",
    "    wrapped_training_data = zip(wrapped_inputs, wrapped_results)\n",
    "    wrapped_test_inputs = [np.reshape(x, (n_dictionary, 1)) for x in wrapped_test_data[0]]\n",
    "    wrapped_test_data_final = zip(wrapped_test_inputs, wrapped_test_data[1])\n",
    "    n_test = len(wrapped_test_inputs)\n",
    "    return wrapped_test_data_final, n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrapped_test_data_final, n_test = format_data(matname)\n",
    "net=network.Network([shl.n_dictionary, n_hidden, 10])\n",
    "net.SGD(training_data=wrapped_training_data,\n",
    "   epochs=30,\n",
    "   mini_batch_size=10,\n",
    "   eta=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Result = ', net.evaluate(wrapped_test_data_final) / len(wrapped_test_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.evaluate(wrapped_test_data_final), len(wrapped_test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etas = np.logspace(-1, 1, 9, base=10)\n",
    "\n",
    "results = []\n",
    "for eta_ in etas:\n",
    "    print(\" ----- learning with eta_ : {} -----\".format(eta_))\n",
    "    wrapped_test_data_final, n_test = format_data(matname)\n",
    "\n",
    "    ## running the network\n",
    "    net=network.Network([shl.n_dictionary, n_hidden, 10])\n",
    "    net.SGD(training_data=wrapped_training_data,\n",
    "       epochs=30,\n",
    "       mini_batch_size=10,\n",
    "       eta=eta_)\n",
    "    results.append(net.evaluate(wrapped_test_data_final) / n_test)\n",
    "\n",
    "plt.plot(etas, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hiddens = 2**np.arange(4, 8)\n",
    "results = []\n",
    "for n_hidden_ in n_hiddens:\n",
    "    print(\" ----- learning with n_hidden : {} -----\".format(n_hidden_))\n",
    "    ## running the network\n",
    "    net=network.Network([shl.n_dictionary, n_hidden_, 10])\n",
    "    net.SGD(training_data=wrapped_training_data,\n",
    "       epochs=30,\n",
    "       mini_batch_size=10,\n",
    "       eta=3.0)\n",
    "    results.append(net.evaluate(wrapped_test_data_final) / n_test)\n",
    "\n",
    "plt.plot(n_hiddens, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_sizes = 2**np.arange(4, 8)\n",
    "results = []\n",
    "for mini_batch_size_ in mini_batch_sizes:\n",
    "    print(\" ----- learning with mini_batch_size : {} -----\".format(mini_batch_size_))\n",
    "    wrapped_test_data_final, n_test = format_data(matname)\n",
    "    ## running the network\n",
    "    net=network.Network([shl.n_dictionary, n_hidden, 10])\n",
    "    net.SGD(training_data=wrapped_training_data,\n",
    "       epochs=30,\n",
    "       mini_batch_size=mini_batch_size_,\n",
    "       eta=3.0)\n",
    "    results.append(net.evaluate(wrapped_test_data_final) / n_test)\n",
    "\n",
    "plt.plot(mini_batch_sizes, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochss = 2**np.arange(4, 8)\n",
    "results = []\n",
    "for epochs_ in epochss:\n",
    "    print(\" ----- learning with epochs : {} -----\".format(epochs_))\n",
    "    wrapped_test_data_final, n_test = format_data(matname)\n",
    "    ## running the network\n",
    "    net=network.Network([shl.n_dictionary, n_hidden, 10])\n",
    "    net.SGD(training_data=wrapped_training_data,\n",
    "       epochs=epochs_,\n",
    "       mini_batch_size=10,\n",
    "       eta=3.0)\n",
    "    results.append(net.evaluate(wrapped_test_data_final) / n_test)\n",
    "\n",
    "plt.plot(epochss, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
